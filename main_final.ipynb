{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc64bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntask 1: importing libraries, creating venv\\ntask 2: face detection & alignment\\ntask 3: local CNN branch\\ntask 4: global ViT branch\\ntask 5: facial region sub branch\\ntask 6: temporal ViT\\ntask 7: fusion and classification\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "task 1: importing libraries, creating venv\n",
    "task 2: face detection & alignment\n",
    "task 3: local CNN branch\n",
    "task 4: global ViT branch\n",
    "task 5: facial region sub branch \n",
    "task 6: temporal ViT (do later)\n",
    "task 7: reducing file size due to miscalculations (196gb -> 24.6gb)\n",
    "task 8: seeing if audio makes sense with lip movement or time lag\n",
    "task 9: fusion and classification\n",
    "\n",
    "should have got facial subregion while face detection so as to reduce computation\n",
    "removing noise from the images as we dont get face detected for every frame\n",
    "doing def for getting features as that is very repitative\n",
    "the cnn features and ViT are very heavy whcih doesnt make sense ig, need to improve on space complexity\n",
    "reduce fps to 10frames/sec would be better as it is enough to capture the nuance (actuualy i just need to avgs acorss the all the frames)\n",
    "adding voice lip reading to see if they are speaking gibirish or not\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0b679",
   "metadata": {},
   "source": [
    "## task 1: importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c56dfd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import timm\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "import torch.nn as nn\n",
    "import mediapipe as mp\n",
    "\n",
    "from torchvision.models import vit_b_16  \n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfec8a0",
   "metadata": {},
   "source": [
    "## task 2: face detection and alignent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2.5GB\"\"\"\n",
    "def extract_faces(video_path, output_subdir, resize_dim=224):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) \n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    " \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % (fps // 10) != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "        frame_count += 1\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(rgb_frame)\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                h, w, _ = rgb_frame.shape\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(w, x2)\n",
    "                y2 = min(h, y2)\n",
    "\n",
    "                if x2 > x1 and y2 > y1:\n",
    "                    face_crop = rgb_frame[y1:y2, x1:x2]\n",
    "                    face_crop = cv2.resize(face_crop, (resize_dim, resize_dim))\n",
    "                    filename = os.path.join(output_subdir, f'face_{saved_count:04d}.jpg')\n",
    "                    cv2.imwrite(filename, cv2.cvtColor(face_crop, cv2.COLOR_RGB2BGR))\n",
    "                    saved_count += 1\n",
    "                else:\n",
    "                    print(f\"skipping invalid box: {(x1, y1, x2, y2)}\")\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253260a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(input_root, output_root):\n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['real', 'fake']:\n",
    "            video_dir = os.path.join(input_root, split, label)\n",
    "            out_dir = os.path.join(output_root, split, label)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            video_files = os.listdir(video_dir)\n",
    "\n",
    "            for video_file in tqdm(video_files, desc=f\"{split}/{label}\"):\n",
    "                video_path = os.path.join(video_dir, video_file)\n",
    "                output_subdir = os.path.join(out_dir, os.path.splitext(video_file)[0])\n",
    "                os.makedirs(output_subdir, exist_ok=True)\n",
    "                extract_faces(video_path, output_subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7365565",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "input_root = 'deepfake_dataset' \n",
    "output_root = 'aligned_faces'  \n",
    "\n",
    "# # Create output directory\n",
    "# os.makedirs(output_root, exist_ok=True)\n",
    "# process_dataset(input_root, output_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1a48a",
   "metadata": {},
   "source": [
    "## task 3+4 local CNN & global ViT brach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_cnn = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transform_vit = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5], \n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Load Pretrained Vision Transformer (e.g., ViT-B/16)\n",
    "feature_extractor_vit = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "feature_extractor_vit.eval()\n",
    "\n",
    "# Load pre-trained ResNet-50 and remove final layers\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "feature_extractor_cnn = nn.Sequential(*list(resnet.children())[:-2])\n",
    "feature_extractor_cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e8ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(input_root, output_root, model, transform):\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['real', 'fake']:\n",
    "            save_dir = os.path.join(output_root, split, label)\n",
    "            label_path = os.path.join(input_root, split, label)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            video_folders = os.listdir(label_path)\n",
    "            \n",
    "            for video in tqdm(video_folders, desc=f\"{split}/{label}\"):\n",
    "                video_path = os.path.join(label_path, video)\n",
    "                features = []\n",
    "\n",
    "                for frame_name in sorted(os.listdir(video_path)):\n",
    "                    frame_path = os.path.join(video_path, frame_name)\n",
    "                    \n",
    "                    img = Image.open(frame_path).convert('RGB')\n",
    "                    input_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        feature = model(input_tensor)  # shape: (1, 2048, 7, 7)\n",
    "                    features.append(feature.squeeze(0)) \n",
    "\n",
    "\n",
    "                video_tensor = torch.stack(features)  # shape: (num_frames, 2048, 7, 7) \n",
    "                                        # update shape to (num_frames, 512, 7, 7), probably good enough to capture features\n",
    "                # Save features\n",
    "                save_path = os.path.join(save_dir, f\"{video}.pt\")\n",
    "                torch.save(video_tensor, save_path)\n",
    "\n",
    "generate_features(\"aligned_faces\", \"extracted_cnn_features\",  feature_extractor_cnn, transform_cnn)  # for cnn\n",
    "generate_features(\"aligned_faces\", \"extracted_ViT_features\", feature_extractor_vit, transform_vit)  # for ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6b80e",
   "metadata": {},
   "source": [
    "## task 5: facial region subranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabda685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and Output Paths\n",
    "input_root = \"aligned_faces\"\n",
    "output_root = {\n",
    "    \"left_eye\": \"extracted_left_eye\",\n",
    "    \"right_eye\": \"extracted_right_eye\",\n",
    "    \"mouth\": \"extracted_mouth\"\n",
    "}\n",
    "stack_size = 10\n",
    "\n",
    "# Create global region folders\n",
    "for region_path in output_root.values():  \n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['real', 'fake']:\n",
    "            os.makedirs(os.path.join(region_path, split, label), exist_ok=True)\n",
    "            \n",
    "# Define facial landmarks\n",
    "FACIAL_REGIONS = {\n",
    "    \"left_eye\": [33, 133, 159, 145, 153, 154, 155, 133],\n",
    "    \"right_eye\": [362, 263, 386, 374, 380, 381, 382, 263],\n",
    "    \"mouth\": [78, 95, 88, 178, 87, 14, 317, 402, 318, 324]\n",
    "}\n",
    "\n",
    "# Face mesh detector\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_region(image, landmarks, region_indices):\n",
    "    h, w, _ = image.shape\n",
    "    points = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in region_indices]\n",
    "\n",
    "    xs, ys = zip(*points)\n",
    "    x_min, x_max = max(min(xs) - 10, 0), min(max(xs) + 10, w)\n",
    "    y_min, y_max = max(min(ys) - 10, 0), min(max(ys) + 10, h)\n",
    "\n",
    "    cropped = image[y_min:y_max, x_min:x_max]\n",
    "    if cropped.size == 0:\n",
    "        return None\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e364b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/real: 100%|██████████| 180/180 [09:09<00:00,  3.05s/it]\n",
      "train/fake: 100%|██████████| 180/180 [11:58<00:00,  3.99s/it]\n",
      "test/real: 100%|██████████| 21/21 [01:26<00:00,  4.13s/it]\n",
      "test/fake: 100%|██████████| 20/20 [01:18<00:00,  3.94s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_facial_region():\n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['real', 'fake']:\n",
    "            input_path = os.path.join(input_root, split, label)\n",
    "            video_folders = os.listdir(input_path)\n",
    "\n",
    "            for video in tqdm(video_folders, desc=f\"{split}/{label}\"):\n",
    "                video_path = os.path.join(input_path, video)\n",
    "                region_buffers = {region: [] for region in FACIAL_REGIONS}\n",
    "                last_landmarks = None\n",
    "\n",
    "                for frame_file in os.listdir(video_path):\n",
    "                    image = cv2.imread(os.path.join(video_path, frame_file))\n",
    "                    if image is None:\n",
    "                        continue\n",
    "\n",
    "                    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    results = face_mesh.process(rgb)\n",
    "\n",
    "                    if results.multi_face_landmarks:\n",
    "                        landmarks = results.multi_face_landmarks[0].landmark\n",
    "                        last_landmarks = landmarks\n",
    "                    elif last_landmarks:\n",
    "                        landmarks = last_landmarks\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    for region, indices in FACIAL_REGIONS.items():\n",
    "                        cropped = crop_region(image, landmarks, indices)\n",
    "                        if cropped is None:\n",
    "                            continue\n",
    "                        resized = cv2.resize(cropped, (64, 64))\n",
    "                        tensor_img = torch.tensor(resized).permute(2, 0, 1).float() / 255.0\n",
    "                        region_buffers[region].append(tensor_img)\n",
    "\n",
    "                # Save the entire buffer once per region\n",
    "                for region in FACIAL_REGIONS:\n",
    "                    if len(region_buffers[region]) == 0:\n",
    "                        continue\n",
    "                    full_stack = torch.stack(region_buffers[region])  # [num_frames, 3, 64, 64]\n",
    "                    save_name = f\"{video}.pt\"\n",
    "                    save_path = os.path.join(output_root[region], split, label, save_name)\n",
    "                    torch.save(full_stack, save_path)\n",
    "# extract_facial_region()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2078d098",
   "metadata": {},
   "source": [
    "## task 6: temporal ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550377b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "data_root = \"aligned_faces\"\n",
    "\n",
    "chunk_size = 30\n",
    "stride = 10  # overlap for smoothness\n",
    "frame_feature_dim = 768  # ViT base output dim\n",
    "temporal_vit_heads = 8\n",
    "temporal_vit_layers = 4\n",
    "\n",
    "\n",
    "# Positional encoding function\n",
    "def get_positional_encoding(seq_len, dim):\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2).float() * -(np.log(10000.0) / dim))\n",
    "    pe = torch.zeros(seq_len, dim)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# Temporal transformer setup\n",
    "temporal_encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=frame_feature_dim,\n",
    "    nhead=temporal_vit_heads,\n",
    "    dim_feedforward=1024,\n",
    "    batch_first=True,\n",
    ")\n",
    "temporal_transformer = nn.TransformerEncoder(\n",
    "    temporal_encoder_layer, num_layers=temporal_vit_layers\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove classification head: keep only the feature extractor part\n",
    "backbone = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "backbone.reset_classifier(0)  # Remove classification head\n",
    "backbone.eval()\n",
    "\n",
    "def extract_vit_features(x):\n",
    "    with torch.no_grad():\n",
    "        x = backbone(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Image transform for ViT input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "base_save_dir = \"extracted_new_temporal_features\"\n",
    "os.makedirs(base_save_dir, exist_ok=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4e046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/real:   3%|▎         | 6/180 [10:24<5:01:45, 104.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[54], line 19\u001b[0m\n",
      "\u001b[0;32m     17\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m     18\u001b[0m     img_tensor \u001b[38;5;241m=\u001b[39m transform(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;32m---> 19\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[43mextract_vit_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (1, 768)\u001b[39;00m\n",
      "\u001b[0;32m     20\u001b[0m     all_frame_feats\u001b[38;5;241m.\u001b[39mappend(feat\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;32m     21\u001b[0m all_frame_feats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(all_frame_feats)  \u001b[38;5;66;03m# (num_frames, 768)\u001b[39;00m\n",
      "\n",
      "Cell \u001b[1;32mIn[52], line 8\u001b[0m, in \u001b[0;36mextract_vit_features\u001b[1;34m(x)\u001b[0m\n",
      "\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_vit_features\u001b[39m(x):\n",
      "\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;32m----> 8\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\timm\\models\\vision_transformer.py:853\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "\u001b[1;32m--> 853\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    854\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n",
      "\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\timm\\models\\vision_transformer.py:834\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m    832\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n",
      "\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 834\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    835\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n",
      "\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\timm\\models\\vision_transformer.py:169\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "\u001b[1;32m--> 169\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[0;32m    170\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n",
      "\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\timm\\models\\vision_transformer.py:93\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     90\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_norm(q), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_norm(k)\n",
      "\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfused_attn:\n",
      "\u001b[1;32m---> 93\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_drop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     98\u001b[0m     q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for split in ['train', 'test']:\n",
    "    for label in ['real', 'fake']:\n",
    "        video_dir = os.path.join(data_root, split, label)\n",
    "        videos = [v for v in os.listdir(video_dir) if os.path.isdir(os.path.join(video_dir, v))]\n",
    "\n",
    "        save_dir = os.path.join(base_save_dir, split, label)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        for video in tqdm(videos, desc=f\"{split}/{label}\"):\n",
    "        frame_dir = os.path.join(video_dir, video)\n",
    "        frame_files = sorted([f for f in os.listdir(frame_dir)])\n",
    "\n",
    "        # Extract features for all frames first (to avoid multiple disk reads per chunk)\n",
    "        all_frame_feats = []\n",
    "        for f in frame_files:\n",
    "            img_path = os.path.join(frame_dir, f)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img_tensor = transform(img).unsqueeze(0)\n",
    "            feat = extract_vit_features(img_tensor)  # (1, 768)\n",
    "            all_frame_feats.append(feat.squeeze(0))\n",
    "        all_frame_feats = torch.stack(all_frame_feats)  # (num_frames, 768)\n",
    "\n",
    "        num_frames = all_frame_feats.size(0)\n",
    "        chunks = []\n",
    "        for start_idx in range(0, num_frames - chunk_size + 1, stride):\n",
    "            chunk_feats = all_frame_feats[start_idx:start_idx + chunk_size]  # (chunk_size, 768)\n",
    "            pe = get_positional_encoding(chunk_size, frame_feature_dim)\n",
    "            chunk_feats_pe = chunk_feats + pe\n",
    "            chunk_feats_pe = chunk_feats_pe.unsqueeze(0)  # (1, chunk_size, 768)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                temporal_out = temporal_transformer(chunk_feats_pe)  # (1, chunk_size, 768)\n",
    "\n",
    "            chunks.append(temporal_out.squeeze(0))  # (chunk_size, 768)\n",
    "\n",
    "        # Concatenate over all chunks to get (num_effective_frames, 768)\n",
    "        chunks = torch.cat(chunks, dim=0)  # (num_frames_effectively_used, 768)\n",
    "\n",
    "        # Add a depth dimension of size 9: (num_frames, 768, 9)\n",
    "        chunks_3d = chunks.unsqueeze(-1).repeat(1, 1, 9)  # (N, 768, 9)\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"{video}.pt\")\n",
    "        torch.save(chunks_3d.cpu(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3036ea5",
   "metadata": {},
   "source": [
    "## task 7: reducing feeature size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# CNN Compression: (2048, 7, 7) → (256, 7, 7)\n",
    "cnn_reducer = nn.Conv2d(2048, 256, kernel_size=1)\n",
    "\n",
    "# ViT Compression: (197, 768) → (197, 96)\n",
    "vit_reducer = nn.Linear(768, 96)\n",
    "\n",
    "# Source directories\n",
    "cnn_dir = \"extracted_cnn_features/test\"\n",
    "vit_dir = \"extracted_vit_features/test\"\n",
    "\n",
    "# Target directories\n",
    "cnn_new_dir = \"extracted_cnn_new_features/test\"\n",
    "vit_new_dir = \"extracted_vit_new_features/test\"\n",
    "\n",
    "# Make sure target directories exist\n",
    "for label in ['real', 'fake']:\n",
    "    os.makedirs(os.path.join(cnn_new_dir, label), exist_ok=True)\n",
    "    os.makedirs(os.path.join(vit_new_dir, label), exist_ok=True)\n",
    "\n",
    "# Set reducers to evaluation mode\n",
    "cnn_reducer.eval()\n",
    "vit_reducer.eval()\n",
    "\n",
    "# Apply compression and save to new locations\n",
    "def reducer():\n",
    "    with torch.no_grad():\n",
    "        for label in ['real', 'fake']:\n",
    "            cnn_label_path = os.path.join(cnn_dir, label)\n",
    "            vit_label_path = os.path.join(vit_dir, label)\n",
    "\n",
    "            cnn_new_label_path = os.path.join(cnn_new_dir, label)\n",
    "            vit_new_label_path = os.path.join(vit_new_dir, label)\n",
    "\n",
    "            for fname in tqdm(os.listdir(cnn_label_path), desc=f\"Compressing {label}\"):\n",
    "                if not fname.endswith(\".pt\"):\n",
    "                    continue\n",
    "\n",
    "                cnn_path = os.path.join(cnn_label_path, fname)\n",
    "                vit_path = os.path.join(vit_label_path, fname)\n",
    "\n",
    "                # Load features\n",
    "                cnn_feat = torch.load(cnn_path)  # (T, 2048, 7, 7)\n",
    "                vit_feat = torch.load(vit_path)  # (T, 197, 768)\n",
    "\n",
    "                # Reduce CNN features\n",
    "                cnn_feat_reduced = cnn_reducer(cnn_feat)  # (T, 256, 7, 7)\n",
    "\n",
    "                # Reduce ViT features\n",
    "                vit_feat_reduced = vit_reducer(vit_feat)  # (T, 197, 96)\n",
    "\n",
    "                # Save to new directories\n",
    "                torch.save(cnn_feat_reduced, os.path.join(cnn_new_label_path, fname))\n",
    "                torch.save(vit_feat_reduced, os.path.join(vit_new_label_path, fname))\n",
    "# reducer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07150403",
   "metadata": {},
   "source": [
    "## task 8: train MLP & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dir = \"extracted_cnn_new_features\"\n",
    "vit_dir = \"extracted_vit_new_features\"\n",
    "left_eye_dir = \"extracted_left_eye_features\"\n",
    "right_eye_dir = \"extracted_right_eye_features\"\n",
    "mouth_dir = \"extracted_mouth_features\"\n",
    "temporal_dir = \"extracted_new_temporal_features\"\n",
    "\n",
    "split = 'train'\n",
    "label_map = {'real': 0, 'fake': 1}\n",
    "batch_size = 32\n",
    "input_dim = (197 * 96) + (256 * 7 * 7) + (3 * 64 * 64 * 3) + (768 * 9)\n",
    "num_epochs = 50\n",
    "lr = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac956779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(split):\n",
    "    samples = []\n",
    "    for label_str, label_val in label_map.items():\n",
    "        cnn_path = os.path.join(cnn_dir, split, label_str)\n",
    "        vit_path = os.path.join(vit_dir, split, label_str)\n",
    "        left_eye_path = os.path.join(left_eye_dir, split, label_str)\n",
    "        right_eye_path = os.path.join(right_eye_dir, split, label_str)\n",
    "        mouth_path = os.path.join(mouth_dir, split, label_str)\n",
    "        temporal_path = os.path.join(temporal_dir, split, label_str)\n",
    "\n",
    "        \n",
    "        for fname in os.listdir(cnn_path):\n",
    "            if fname.endswith(\".pt\"):\n",
    "                cnn_file = os.path.join(cnn_path, fname)\n",
    "                vit_file = os.path.join(vit_path, fname)\n",
    "                left_eye_file = os.path.join(left_eye_path, fname)\n",
    "                right_eye_file = os.path.join(right_eye_path, fname)\n",
    "                mouth_file = os.path.join(mouth_path, fname)\n",
    "                temporal_file = os.path.join(temporal_path, fname)\n",
    "\n",
    "                if os.path.exists(vit_file):\n",
    "                    samples.append((cnn_file, vit_file, left_eye_file, right_eye_file, mouth_file, temporal_file, label_val))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(file_list, batch_size):\n",
    "    for i in range(0, len(file_list), batch_size):\n",
    "        batch = file_list[i:i+batch_size]\n",
    "        features = []\n",
    "        labels = []\n",
    "        for cnn_path, vit_path, left_eye_path, right_eye_path, mouth_path, temporal_path, label in batch:\n",
    "            cnn_feat = torch.load(cnn_path)                 # (num_frames, 2048, 7, 7)\n",
    "            vit_feat = torch.load(vit_path)                 # (num_frames, 197, 96)\n",
    "            left_eye_feat = torch.load(left_eye_path)       # (num_frames, 3, 64, 64)\n",
    "            right_eye_feat = torch.load(right_eye_path)     # (num_frames, 3, 64, 64)\n",
    "            mouth_feat = torch.load(mouth_path)             # (num_frames, 3, 64, 64)\n",
    "            temporal_feat = torch.load(temporal_path)             # (num_frames, 3, 64, 64)\n",
    "\n",
    "            cnn_avg = cnn_feat.mean(dim=0)         # (2048, 7, 7)\n",
    "            vit_avg = vit_feat.mean(dim=0)         # (197, 96)\n",
    "            left_eye_avg = left_eye_feat.mean(dim=0)         # (64, 64)\n",
    "            right_eye_avg = right_eye_feat.mean(dim=0)         # (64, 64)\n",
    "            mouth_avg = mouth_feat.mean(dim=0)         # (64, 64)\n",
    "            temporal_avg = temporal_feat.mean(dim=0)         # (64, 64)\n",
    "\n",
    "            fused = torch.cat([cnn_avg.flatten(), vit_avg.flatten(), left_eye_avg.flatten(), right_eye_avg.flatten(), mouth_avg.flatten(), temporal_avg.flatten()], dim=0)\n",
    "            features.append(fused)\n",
    "            labels.append(label)\n",
    "\n",
    "        features = torch.stack(features)\n",
    "        features = F.batch_norm(features, running_mean=None, running_var=None, training=True)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # shape (batch_size, 1)\n",
    "        yield features, labels\n",
    "        \n",
    "file_list_train = get_file_list('train')\n",
    "file_list_test = get_file_list('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46178c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "\n",
    "    nn.Linear(512, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    \n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a33a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Average Loss: 0.5275\n",
      "Epoch 2 complete. Average Loss: 0.5211\n",
      "Epoch 3 complete. Average Loss: 0.5197\n",
      "Epoch 4 complete. Average Loss: 0.5217\n",
      "Epoch 5 complete. Average Loss: 0.5099\n",
      "Epoch 6 complete. Average Loss: 0.5119\n",
      "Epoch 7 complete. Average Loss: 0.5151\n",
      "Epoch 8 complete. Average Loss: 0.5081\n",
      "Epoch 9 complete. Average Loss: 0.5136\n",
      "Epoch 10 complete. Average Loss: 0.5135\n",
      "Epoch 11 complete. Average Loss: 0.5114\n",
      "Epoch 12 complete. Average Loss: 0.5119\n",
      "Epoch 13 complete. Average Loss: 0.5108\n",
      "Epoch 14 complete. Average Loss: 0.5091\n",
      "Epoch 15 complete. Average Loss: 0.5017\n",
      "Epoch 16 complete. Average Loss: 0.5108\n",
      "Epoch 17 complete. Average Loss: 0.5053\n",
      "Epoch 18 complete. Average Loss: 0.5016\n",
      "Epoch 19 complete. Average Loss: 0.5077\n",
      "Epoch 20 complete. Average Loss: 0.5010\n",
      "Epoch 21 complete. Average Loss: 0.4954\n",
      "Epoch 22 complete. Average Loss: 0.4988\n",
      "Epoch 23 complete. Average Loss: 0.4971\n",
      "Epoch 24 complete. Average Loss: 0.5014\n",
      "Epoch 25 complete. Average Loss: 0.4925\n",
      "Epoch 26 complete. Average Loss: 0.4890\n",
      "Epoch 27 complete. Average Loss: 0.4961\n",
      "Epoch 28 complete. Average Loss: 0.5005\n",
      "Epoch 29 complete. Average Loss: 0.4940\n",
      "Epoch 30 complete. Average Loss: 0.4843\n",
      "Epoch 31 complete. Average Loss: 0.4922\n",
      "Epoch 32 complete. Average Loss: 0.4856\n",
      "Epoch 33 complete. Average Loss: 0.4875\n",
      "Epoch 34 complete. Average Loss: 0.4877\n",
      "Epoch 35 complete. Average Loss: 0.4884\n",
      "Epoch 36 complete. Average Loss: 0.4812\n",
      "Epoch 37 complete. Average Loss: 0.4868\n",
      "Epoch 38 complete. Average Loss: 0.4806\n",
      "Epoch 39 complete. Average Loss: 0.4828\n",
      "Epoch 40 complete. Average Loss: 0.4760\n",
      "Epoch 41 complete. Average Loss: 0.4846\n",
      "Epoch 42 complete. Average Loss: 0.4822\n",
      "Epoch 43 complete. Average Loss: 0.4820\n",
      "Epoch 44 complete. Average Loss: 0.4764\n",
      "Epoch 45 complete. Average Loss: 0.4819\n",
      "Epoch 46 complete. Average Loss: 0.4767\n",
      "Epoch 47 complete. Average Loss: 0.4775\n",
      "Epoch 48 complete. Average Loss: 0.4758\n",
      "Epoch 49 complete. Average Loss: 0.4805\n",
      "Epoch 50 complete. Average Loss: 0.4767\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in batch_generator(file_list_train, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(file_list_train)\n",
    "    print(f\"Epoch {epoch+1} complete. Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model7.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.5000\n",
      "Test Precision: 0.5000\n",
      "Test Recall:    0.6500\n",
      "Test F1-score:  0.5652\n",
      "Test ROC AUC:   0.5025\n",
      "Confusion Matrix:\n",
      "   [[ 7 13]\n",
      " [ 7 13]]\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# model.load_state_dict(torch.load(\"model5.pth\"))\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_prob = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in batch_generator(file_list_test, batch_size):\n",
    "        outputs = model(X_batch)\n",
    "        probs = outputs.squeeze(1).numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "        y_prob.extend(probs.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_true.extend(y_batch.squeeze(1).numpy().tolist())\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall:    {recall:.4f}\")\n",
    "print(f\"Test F1-score:  {f1:.4f}\")\n",
    "print(f\"Test ROC AUC:   {roc_auc:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n   {cm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30368eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "model 3:\n",
    "\n",
    "Test Accuracy:  0.4750\n",
    "Test Precision: 0.4872\n",
    "Test Recall:    0.9500\n",
    "Test F1-score:  0.6441\n",
    "Test ROC AUC:   0.4825\n",
    "Confusion Matrix:\n",
    "   [[ 0 20]\n",
    " [ 1 19]]\n",
    "\n",
    "model 4:\n",
    "Test Accuracy:  0.6000\n",
    "Test Precision: 0.5588\n",
    "Test Recall:    0.9500\n",
    "Test F1-score:  0.7037\n",
    "Test ROC AUC:   0.6825\n",
    "Confusion Matrix:\n",
    "   [[ 5 15]\n",
    " [ 1 19]]\n",
    "\n",
    " model 6:\n",
    " Test Accuracy:  0.5000\n",
    "Test Precision: 0.5000\n",
    "Test Recall:    0.6500\n",
    "Test F1-score:  0.5652\n",
    "Test ROC AUC:   0.4975\n",
    "Confusion Matrix:\n",
    "   [[ 7 13]\n",
    " [ 7 13]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696addae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 - Standard deviation from 0.5: 0.0486\n",
      "Case 2 - Standard deviation from 0.5: 0.0717\n",
      "Case 3 - Standard deviation from 0.5: 0.0717\n",
      "Case 4 - Standard deviation from 0.5: 0.0580\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Case 1\n",
    "confidences1 = [\n",
    "    0.5699, 0.5338, 0.5913, 0.5609, 0.5096, 0.5951, 0.5687, 0.4923, 0.5180, 0.4610,\n",
    "    0.5308, 0.4856, 0.4915, 0.5454, 0.5254, 0.5849, 0.5008, 0.4538, 0.4192, 0.4103,\n",
    "    0.4825, 0.5545, 0.4291, 0.5627, 0.5094, 0.5163, 0.4627, 0.4958, 0.5376, 0.6003,\n",
    "    0.5290, 0.5471, 0.5184, 0.5077, 0.5442, 0.4670, 0.5532, 0.4784, 0.5144, 0.4241\n",
    "]\n",
    "\n",
    "# Case 2\n",
    "confidences2 = [\n",
    "    0.4792, 0.4165, 0.3935, 0.4888, 0.4869, 0.5575, 0.4369, 0.4190, 0.3915, 0.5161,\n",
    "    0.4184, 0.4645, 0.3272, 0.4421, 0.5519, 0.4179, 0.4256, 0.3231, 0.3035, 0.3467,\n",
    "    0.3666, 0.5159, 0.3462, 0.5549, 0.5471, 0.5165, 0.3977, 0.4066, 0.4282, 0.5677,\n",
    "    0.4377, 0.4827, 0.4282, 0.5412, 0.5263, 0.4085, 0.5496, 0.4776, 0.5307, 0.4417\n",
    "]\n",
    "\n",
    "# Case 3 (example, replace with your actual third list if different)\n",
    "confidences3 = [\n",
    "    0.4792, 0.4165, 0.3935, 0.4888, 0.4869, 0.5575, 0.4369, 0.4190, 0.3915, 0.5161,\n",
    "    0.4184, 0.4645, 0.3272, 0.4421, 0.5519, 0.4179, 0.4256, 0.3231, 0.3035, 0.3467,\n",
    "    0.3666, 0.5159, 0.3462, 0.5549, 0.5471, 0.5165, 0.3977, 0.4066, 0.4282, 0.5677,\n",
    "    0.4377, 0.4827, 0.4282, 0.5412, 0.5263, 0.4085, 0.5496, 0.4776, 0.5307, 0.4417\n",
    "]\n",
    "confidences4 = [\n",
    "    0.5270, 0.4997, 0.4106, 0.5195, 0.5833, 0.5680, 0.4729, 0.5273, 0.5583, 0.6504,\n",
    "    0.5328, 0.6440, 0.4677, 0.4743, 0.4959, 0.4630, 0.5865, 0.4870, 0.4978, 0.4622,\n",
    "    0.5110, 0.6130, 0.4501, 0.4734, 0.5093, 0.6621, 0.5715, 0.5819, 0.4787, 0.5733,\n",
    "    0.4888, 0.4677, 0.4752, 0.4618, 0.5163, 0.4736, 0.5532, 0.5404, 0.5659, 0.5239\n",
    "]\n",
    "for i, confs in enumerate([confidences1, confidences2, confidences3, confidences4], 1):\n",
    "    if len(confs) == 0:\n",
    "        print(f\"Case {i}: No data\")\n",
    "        continue\n",
    "    spread = np.std(np.array(confs) - 0.5)\n",
    "    print(f\"Case {i} - Standard deviation from 0.5: {spread:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
