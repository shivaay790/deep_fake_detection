{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fd3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntask 1: importing libraries, creating venv\\ntask 2: face detection & alignment\\ntask 3: local CNN branch\\ntask 4: global ViT branch\\ntask 5: facial region sub branch\\ntask 6: temporal ViT\\ntask 7: fusion and classification\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "task 1: importing libraries, creating venv\n",
    "task 2: face detection & alignment\n",
    "task 3: local CNN branch\n",
    "task 4: global ViT branch\n",
    "task 5: facial region sub branch \n",
    "task 6: temporal ViT (do later)\n",
    "task 7: reducing file size due to miscalculations (196gb -> 24.6gb)\n",
    "task 8: seeing if audio makes sense with lip movement or time lag\n",
    "task 9: fusion and classification\n",
    "\n",
    "should have got facial subregion while face detection so as to reduce computation\n",
    "removing noise from the images as we dont get face detected for every frame\n",
    "doing def for getting features as that is very repitative\n",
    "the cnn features and ViT are very heavy whcih doesnt make sense ig, need to improve on space req\n",
    "adding voice lip reading to see if they are speaking gibirish or not\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075e17e",
   "metadata": {},
   "source": [
    "## task 1: importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c15bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivaay Dhondiyal\\Desktop\\shivaay\\coding\\2_development\\projects\\6_Gans\\global_local\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import timm\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "import torch.nn as nn\n",
    "import mediapipe as mp\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193a24b",
   "metadata": {},
   "source": [
    "## task 2: face detection and alignent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f313e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces(video_path, output_subdir, resize_dim=224):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) \n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    " \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % (fps // 10) != 0:\n",
    "            frame_count += 1\n",
    "            continue\n",
    "        frame_count += 1\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(rgb_frame)\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "\n",
    "        if boxes is not None:\n",
    "            for box in boxes:\n",
    "                h, w, _ = rgb_frame.shape\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(w, x2)\n",
    "                y2 = min(h, y2)\n",
    "\n",
    "                if x2 > x1 and y2 > y1:\n",
    "                    face_crop = rgb_frame[y1:y2, x1:x2]\n",
    "                    face_crop = cv2.resize(face_crop, (resize_dim, resize_dim))\n",
    "                    filename = os.path.join(output_subdir, f'face_{saved_count:04d}.jpg')\n",
    "                    cv2.imwrite(filename, cv2.cvtColor(face_crop, cv2.COLOR_RGB2BGR))\n",
    "                    saved_count += 1\n",
    "                else:\n",
    "                    print(f\"skipping invalid box\")\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(input_root, output_root):\n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['real', 'fake']:\n",
    "            video_dir = os.path.join(input_root, split, label)\n",
    "            out_dir = os.path.join(output_root, split, label)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            video_files = os.listdir(video_dir)\n",
    "\n",
    "            for video_file in tqdm(video_files, desc=f\"{split}/{label}\"):\n",
    "                video_path = os.path.join(video_dir, video_file)\n",
    "                output_subdir = os.path.join(out_dir, os.path.splitext(video_file)[0])\n",
    "                os.makedirs(output_subdir, exist_ok=True)\n",
    "                extract_faces(video_path, output_subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a70c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=False)\n",
    "\n",
    "input_root = 'deepfake_dataset' \n",
    "output_root = 'aligned_faces'  \n",
    "\n",
    "# # Create output directory\n",
    "# os.makedirs(output_root, exist_ok=True)\n",
    "# process_dataset(input_root, output_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d22da",
   "metadata": {},
   "source": [
    "## task 3 local CNN brach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f97b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html\n",
    "                         std=[0.229, 0.224, 0.225])  # the values for transform values are picked from the above link\n",
    "])\n",
    "\n",
    "# Load pre trained resnet 50 and remove final layers\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "feature_extractor.eval()\n",
    "\n",
    "input_root = 'aligned_faces'\n",
    "output_root = 'extracted_cnn_features'\n",
    "os.makedirs(output_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19da334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/real:   0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/real: 100%|██████████| 180/180 [1:27:32<00:00, 29.18s/it]\n",
      "train/fake: 100%|██████████| 180/180 [1:13:15<00:00, 24.42s/it]\n",
      "test/real: 100%|██████████| 21/21 [09:01<00:00, 25.80s/it]\n",
      "test/fake: 100%|██████████| 20/20 [08:08<00:00, 24.42s/it]\n"
     ]
    }
   ],
   "source": [
    "def generate_cnn_features():\n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['real', 'fake']:\n",
    "            save_dir = os.path.join(output_root, split, label)\n",
    "            label_path = os.path.join(input_root, split, label)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            video_folders = os.listdir(label_path)\n",
    "            \n",
    "            for video in tqdm(video_folders, desc=f\"{split}/{label}\"):\n",
    "                video_path = os.path.join(label_path, video)\n",
    "                features = []\n",
    "\n",
    "                for frame_name in sorted(os.listdir(video_path)):\n",
    "                    frame_path = os.path.join(video_path, frame_name)\n",
    "                    \n",
    "                    img = Image.open(frame_path).convert('RGB')\n",
    "                    input_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        feature = feature_extractor(input_tensor)  # shape: (1, 2048, 7, 7)\n",
    "                    features.append(feature.squeeze(0))\n",
    "\n",
    "                if features:\n",
    "                    video_tensor = torch.stack(features)  # shape: (num_frames, 2048, 7, 7) \n",
    "                                            # update shape to (num_frames, 512, 7, 7), probably good enough to capture features\n",
    "                    save_path = os.path.join(save_dir, f\"{video}.pt\")\n",
    "                    torch.save(video_tensor, save_path)\n",
    "# generate_cnn_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b039be",
   "metadata": {},
   "source": [
    "## task 4: global ViT ranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340402af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "model.eval().to(\"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],  \n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "input_root = 'aligned_faces'  \n",
    "output_root = 'extracted_ViT_features'  \n",
    "os.makedirs(output_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c5db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/real:   0%|          | 0/180 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/real: 100%|██████████| 180/180 [3:11:12<00:00, 63.74s/it]   \n",
      "train/fake: 100%|██████████| 180/180 [2:44:48<00:00, 54.94s/it]  \n",
      "test/real: 100%|██████████| 21/21 [20:33<00:00, 58.72s/it] \n",
      "test/fake: 100%|██████████| 20/20 [32:14<00:00, 96.73s/it] \n"
     ]
    }
   ],
   "source": [
    "def generate_ViT_features():\n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['real', 'fake']:\n",
    "            label_input_path = os.path.join(input_root, split, label)\n",
    "            label_output_path = os.path.join(output_root, split, label)\n",
    "\n",
    "            video_folders = os.listdir(label_input_path)\n",
    "\n",
    "            for video in tqdm(video_folders, desc=f\"{split}/{label}\"):\n",
    "                video_input_path = os.path.join(label_input_path, video)\n",
    "                os.makedirs(label_output_path, exist_ok=True)\n",
    "\n",
    "                if not os.path.isdir(video_input_path):\n",
    "                    continue\n",
    "\n",
    "                video_features = []\n",
    "\n",
    "                for frame_file in sorted(os.listdir(video_input_path)):\n",
    "                    frame_path = os.path.join(video_input_path, frame_file)\n",
    "\n",
    "                    img = Image.open(frame_path).convert('RGB')\n",
    "                    input_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        features = model.forward_features(input_tensor)  # [1, 197, 768]\n",
    "\n",
    "                    video_features.append(features.squeeze(0).cpu())\n",
    "\n",
    "                if video_features:\n",
    "                    video_tensor = torch.stack(video_features)  # shape: [num_frames, 197, 768]\n",
    "                    save_path = os.path.join(label_output_path, f\"{video}.pt\")\n",
    "\n",
    "                    torch.save(video_tensor, save_path)\n",
    "                    \n",
    "# generate_ViT_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff84a38",
   "metadata": {},
   "source": [
    "## task 5: facial region subranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafa42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_root = \"aligned_faces\"\n",
    "output_root = {\n",
    "    \"left_eye\": \"extracted_left_eye\",\n",
    "    \"right_eye\": \"extracted_right_eye\",\n",
    "    \"mouth\": \"extracted_mouth\"\n",
    "}\n",
    "stack_size = 10\n",
    "\n",
    "for region_path in output_root.values():  \n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['real', 'fake']:\n",
    "            os.makedirs(os.path.join(region_path, split, label), exist_ok=True)\n",
    "\n",
    "FACIAL_REGIONS = {\n",
    "    \"left_eye\": [33, 133, 159, 145, 153, 154, 155, 133],\n",
    "    \"right_eye\": [362, 263, 386, 374, 380, 381, 382, 263],\n",
    "    \"mouth\": [78, 95, 88, 178, 87, 14, 317, 402, 318, 324]\n",
    "}\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b68938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_region(image, landmarks, region_indices):\n",
    "    h, w, _ = image.shape\n",
    "    points = [(int(landmarks[i].x * w), int(landmarks[i].y * h)) for i in region_indices]\n",
    "\n",
    "    xs, ys = zip(*points)\n",
    "    x_min, x_max = max(min(xs) - 10, 0), min(max(xs) + 10, w)\n",
    "    y_min, y_max = max(min(ys) - 10, 0), min(max(ys) + 10, h)\n",
    "\n",
    "    cropped = image[y_min:y_max, x_min:x_max]\n",
    "    if cropped.size == 0:\n",
    "        return None\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c03a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/real: 100%|██████████| 180/180 [09:09<00:00,  3.05s/it]\n",
      "train/fake: 100%|██████████| 180/180 [11:58<00:00,  3.99s/it]\n",
      "test/real: 100%|██████████| 21/21 [01:26<00:00,  4.13s/it]\n",
      "test/fake: 100%|██████████| 20/20 [01:18<00:00,  3.94s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_facial_features():\n",
    "    for split in ['train', 'test']:\n",
    "        for label in ['real', 'fake']:\n",
    "            input_path = os.path.join(input_root, split, label)\n",
    "            video_folders = [v for v in os.listdir(input_path) if os.path.isdir(os.path.join(input_path, v))]\n",
    "\n",
    "            for video in tqdm(video_folders, desc=f\"{split}/{label}\"):\n",
    "                video_path = os.path.join(input_path, video)\n",
    "                region_buffers = {region: [] for region in FACIAL_REGIONS}\n",
    "                last_landmarks = None\n",
    "\n",
    "                for frame_file in sorted(os.listdir(video_path)):\n",
    "                    if not frame_file.lower().endswith('.jpg'):\n",
    "                        continue\n",
    "                    image = cv2.imread(os.path.join(video_path, frame_file))\n",
    "                    if image is None:\n",
    "                        continue\n",
    "\n",
    "                    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    results = face_mesh.process(rgb)\n",
    "\n",
    "                    if results.multi_face_landmarks:\n",
    "                        landmarks = results.multi_face_landmarks[0].landmark\n",
    "                        last_landmarks = landmarks\n",
    "                    elif last_landmarks:\n",
    "                        landmarks = last_landmarks\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    for region, indices in FACIAL_REGIONS.items():\n",
    "                        cropped = crop_region(image, landmarks, indices)\n",
    "                        if cropped is None:\n",
    "                            continue\n",
    "                        resized = cv2.resize(cropped, (64, 64))\n",
    "                        tensor_img = torch.tensor(resized).permute(2, 0, 1).float() / 255.0\n",
    "                        region_buffers[region].append(tensor_img)\n",
    "\n",
    "                for region in FACIAL_REGIONS:\n",
    "                    if len(region_buffers[region]) == 0:\n",
    "                        continue\n",
    "                    full_stack = torch.stack(region_buffers[region])  # [num_frames, 3, 64, 64]\n",
    "                    save_name = f\"{video}.pt\"\n",
    "                    save_path = os.path.join(output_root[region], split, label, save_name)\n",
    "                    torch.save(full_stack, save_path)\n",
    "                    \n",
    "extract_facial_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a6b5c",
   "metadata": {},
   "source": [
    "## task 6: temporal ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "data_root = \"aligned_faces\"\n",
    "\n",
    "chunk_size = 30\n",
    "stride = 10  \n",
    "frame_feature_dim = 768 \n",
    "temporal_vit_heads = 8\n",
    "temporal_vit_layers = 4\n",
    "\n",
    "\n",
    "def get_positional_encoding(seq_len, dim):\n",
    "    position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2).float() * -(np.log(10000.0) / dim))\n",
    "    pe = torch.zeros(seq_len, dim)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "temporal_encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=frame_feature_dim,\n",
    "    nhead=temporal_vit_heads,\n",
    "    dim_feedforward=1024,\n",
    "    batch_first=True,\n",
    ")\n",
    "temporal_transformer = nn.TransformerEncoder(\n",
    "    temporal_encoder_layer, num_layers=temporal_vit_layers\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e786f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "backbone.reset_classifier(0) \n",
    "backbone.eval()\n",
    "\n",
    "def extract_vit_features(x):\n",
    "    with torch.no_grad():\n",
    "        x = backbone(x)\n",
    "    return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "base_save_dir = \"extracted_temporal_features\"\n",
    "os.makedirs(base_save_dir, exist_ok=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f86025",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'test']:\n",
    "    for label in ['real', 'fake']:\n",
    "        video_dir = os.path.join(data_root, split, label)\n",
    "        videos = [v for v in os.listdir(video_dir) if os.path.isdir(os.path.join(video_dir, v))]\n",
    "\n",
    "        save_dir = os.path.join(base_save_dir, split, label)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        for video in tqdm(videos, desc=f\"{split}/{label}\"):\n",
    "        frame_dir = os.path.join(video_dir, video)\n",
    "        frame_files = sorted([f for f in os.listdir(frame_dir)])\n",
    "\n",
    "        all_frame_feats = []\n",
    "        for f in frame_files:\n",
    "            img_path = os.path.join(frame_dir, f)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img_tensor = transform(img).unsqueeze(0)\n",
    "            feat = extract_vit_features(img_tensor)  # (1, 768)\n",
    "            all_frame_feats.append(feat.squeeze(0))\n",
    "        all_frame_feats = torch.stack(all_frame_feats)  # (num_frames, 768)\n",
    "\n",
    "        num_frames = all_frame_feats.size(0)\n",
    "        chunks = []\n",
    "        for start_idx in range(0, num_frames - chunk_size + 1, stride):\n",
    "            chunk_feats = all_frame_feats[start_idx:start_idx + chunk_size]  \n",
    "            pe = get_positional_encoding(chunk_size, frame_feature_dim)\n",
    "            chunk_feats_pe = chunk_feats + pe\n",
    "            chunk_feats_pe = chunk_feats_pe.unsqueeze(0) \n",
    "\n",
    "            with torch.no_grad():\n",
    "                temporal_out = temporal_transformer(chunk_feats_pe) \n",
    "\n",
    "            chunks.append(temporal_out.squeeze(0))  \n",
    "\n",
    "        chunks = torch.cat(chunks, dim=0) # remove this, get dim of (N,768*9) and then reshape to (N, 768, 9) insted of reapting values\n",
    "        chunks_3d = chunks.unsqueeze(-1).repeat(1, 1, 9)  # (N, 768, 9)\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"{video}.pt\")\n",
    "        torch.save(chunks_3d.cpu(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af44b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/real:   2%|▏         | 4/180 [00:00<00:08, 19.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected shape in 01__hugging_happy.pt: torch.Size([1260, 768, 9])\n",
      "Unexpected shape in 01__kitchen_pan.pt: torch.Size([810, 768, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train/real: 100%|██████████| 180/180 [00:09<00:00, 19.24it/s]\n",
      "train/fake: 100%|██████████| 180/180 [00:07<00:00, 23.97it/s]\n",
      "test/real: 100%|██████████| 20/20 [00:00<00:00, 22.97it/s]\n",
      "test/fake: 100%|██████████| 20/20 [00:00<00:00, 24.73it/s]\n"
     ]
    }
   ],
   "source": [
    "input_root = \"extracted_temporal_features\"\n",
    "output_root = \"extracted_new_temporal_features\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    for label in ['real', 'fake']:\n",
    "        input_dir = os.path.join(input_root, split, label)\n",
    "        output_dir = os.path.join(output_root, split, label)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for fname in tqdm(os.listdir(input_dir), desc=f\"{split}/{label}\"):\n",
    "            if not fname.endswith(\".pt\"):\n",
    "                continue\n",
    "\n",
    "            input_path = os.path.join(input_dir, fname)\n",
    "            output_path = os.path.join(output_dir, fname)\n",
    "\n",
    "            data = torch.load(input_path)\n",
    "\n",
    "            if data.ndim == 1:\n",
    "                print(f\"Skipping (1D) {fname}, shape: {data.shape}\")\n",
    "                continue\n",
    "\n",
    "            if data.ndim == 2:\n",
    "                # Expected shape: (num_frames, 768)\n",
    "                expanded = data.unsqueeze(-1).repeat(1, 1, 9)  # (num_frames, 768, 9)\n",
    "                torch.save(expanded, output_path)\n",
    "            else:\n",
    "                print(f\"unexpected shape in {fname}: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317caa9",
   "metadata": {},
   "source": [
    "## task 7: reducing feeature size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e4734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Compression: (2048, 7, 7) → (256, 7, 7)\n",
    "cnn_reducer = nn.Conv2d(2048, 256, kernel_size=1)\n",
    "# ViT Compression: (197, 768) → (197, 96)\n",
    "vit_reducer = nn.Linear(768, 96)\n",
    "\n",
    "cnn_dir = \"extracted_cnn_features/test\"\n",
    "vit_dir = \"extracted_vit_features/test\"\n",
    "cnn_new_dir = \"extracted_cnn_new_features/test\"\n",
    "vit_new_dir = \"extracted_vit_new_features/test\"\n",
    "for label in ['real', 'fake']:\n",
    "    os.makedirs(os.path.join(cnn_new_dir, label), exist_ok=True)\n",
    "    os.makedirs(os.path.join(vit_new_dir, label), exist_ok=True)\n",
    "\n",
    "\n",
    "cnn_reducer.eval()\n",
    "vit_reducer.eval()\n",
    "\n",
    "def reducer():\n",
    "    with torch.no_grad():\n",
    "        for label in ['real', 'fake']:\n",
    "            cnn_label_path = os.path.join(cnn_dir, label)\n",
    "            vit_label_path = os.path.join(vit_dir, label)\n",
    "\n",
    "            cnn_new_label_path = os.path.join(cnn_new_dir, label)\n",
    "            vit_new_label_path = os.path.join(vit_new_dir, label)\n",
    "\n",
    "            for fname in tqdm(os.listdir(cnn_label_path), desc=f\"Compressing {label}\"):\n",
    "                if not fname.endswith(\".pt\"):\n",
    "                    continue\n",
    "\n",
    "                cnn_path = os.path.join(cnn_label_path, fname)\n",
    "                vit_path = os.path.join(vit_label_path, fname)\n",
    "                cnn_feat = torch.load(cnn_path)  # (T, 2048, 7, 7)\n",
    "                vit_feat = torch.load(vit_path)  # (T, 197, 768)\n",
    "\n",
    "                cnn_feat_reduced = cnn_reducer(cnn_feat)  # (T, 256, 7, 7)\n",
    "                vit_feat_reduced = vit_reducer(vit_feat)  # (T, 197, 96)\n",
    "\n",
    "                torch.save(cnn_feat_reduced, os.path.join(cnn_new_label_path, fname))\n",
    "                torch.save(vit_feat_reduced, os.path.join(vit_new_label_path, fname))\n",
    "# reducer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fe3b7",
   "metadata": {},
   "source": [
    "## task 8: train MLP & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018d958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dir = \"extracted_cnn_new_features\"\n",
    "vit_dir = \"extracted_vit_new_features\"\n",
    "left_eye_dir = \"extracted_left_eye_features\"\n",
    "right_eye_dir = \"extracted_right_eye_features\"\n",
    "mouth_dir = \"extracted_mouth_features\"\n",
    "temporal_dir = \"extracted_new_temporal_features\"\n",
    "\n",
    "split = 'train'\n",
    "label_map = {'real': 0, 'fake': 1}\n",
    "batch_size = 180\n",
    "input_dim = (197 * 96) + (256 * 7 * 7) + (3 * 64 * 64 * 3) + (768 * 9)\n",
    "num_epochs = 20\n",
    "lr = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c2c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(split):\n",
    "    samples = []\n",
    "    for label_str, label_val in label_map.items():\n",
    "        cnn_path = os.path.join(cnn_dir, split, label_str)\n",
    "        vit_path = os.path.join(vit_dir, split, label_str)\n",
    "        left_eye_path = os.path.join(left_eye_dir, split, label_str)\n",
    "        right_eye_path = os.path.join(right_eye_dir, split, label_str)\n",
    "        mouth_path = os.path.join(mouth_dir, split, label_str)\n",
    "        temporal_path = os.path.join(temporal_dir, split, label_str)\n",
    "\n",
    "        \n",
    "        for fname in os.listdir(cnn_path):\n",
    "            if fname.endswith(\".pt\"):\n",
    "                cnn_file = os.path.join(cnn_path, fname)\n",
    "                vit_file = os.path.join(vit_path, fname)\n",
    "                left_eye_file = os.path.join(left_eye_path, fname)\n",
    "                right_eye_file = os.path.join(right_eye_path, fname)\n",
    "                mouth_file = os.path.join(mouth_path, fname)\n",
    "                temporal_file = os.path.join(temporal_path, fname)\n",
    "\n",
    "                if os.path.exists(vit_file):\n",
    "                    samples.append((cnn_file, vit_file, left_eye_file, right_eye_file, mouth_file, temporal_file, label_val))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c80788fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_generator(file_list, batch_size, scaler):\n",
    "    for i in range(0, len(file_list), batch_size):\n",
    "        batch = file_list[i:i+batch_size]\n",
    "        features = []\n",
    "        labels = []\n",
    "        for cnn_path, vit_path, left_eye_path, right_eye_path, mouth_path, temporal_path, label in batch:\n",
    "            cnn_feat = torch.load(cnn_path)                 # (num_frames, 2048, 7, 7)\n",
    "            vit_feat = torch.load(vit_path)                 # (num_frames, 197, 96)\n",
    "            left_eye_feat = torch.load(left_eye_path)       # (num_frames, 3, 64, 64)\n",
    "            right_eye_feat = torch.load(right_eye_path)     # (num_frames, 3, 64, 64)\n",
    "            mouth_feat = torch.load(mouth_path)             # (num_frames, 3, 64, 64)\n",
    "            temporal_feat = torch.load(temporal_path)             # (num_frames, 768, 9)\n",
    "\n",
    "            cnn_avg = cnn_feat.mean(dim=0)         # (2048, 7, 7)\n",
    "            vit_avg = vit_feat.mean(dim=0)         # (197, 96)\n",
    "            left_eye_avg = left_eye_feat.mean(dim=0)         # (64, 64)\n",
    "            right_eye_avg = right_eye_feat.mean(dim=0)         # (64, 64)\n",
    "            mouth_avg = mouth_feat.mean(dim=0)         # (64, 64)\n",
    "            temporal_avg = temporal_feat.mean(dim=0)         # (768, 9)\n",
    "\n",
    "            fused = torch.cat([cnn_avg.flatten(), vit_avg.flatten(), left_eye_avg.flatten(), right_eye_avg.flatten(), mouth_avg.flatten(), temporal_avg.flatten()], dim=0)\n",
    "            features.append(fused)\n",
    "            labels.append(label)\n",
    "\n",
    "        features = torch.stack(features)\n",
    "        if scaler is not None:\n",
    "            features = torch.tensor(scaler.transform(features.numpy()), dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # shape (batch_size, 1)\n",
    "        yield features, labels\n",
    "        \n",
    "file_list_train = get_file_list('train')\n",
    "file_list_test = get_file_list('test')\n",
    "\n",
    "all_train_features = []\n",
    "for X_batch, _ in batch_generator(file_list_train, batch_size, scaler=None):\n",
    "    all_train_features.append(X_batch)\n",
    "all_train_features = torch.cat(all_train_features, dim=0).numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ede062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "\n",
    "    nn.Linear(512, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    \n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96676f76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      5\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbatch_generator\u001b[49m(file_list_train, batch_size, scaler\u001b[38;5;241m=\u001b[39mscaler):\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_generator' is not defined"
     ]
    }
   ],
   "source": [
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.75)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in batch_generator(file_list_train, batch_size, scaler=scaler):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(file_list_train)\n",
    "    print(f\"epoch {epoch+1} complete. avg loss: {avg_loss:.4f}\")\n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3ebdf018",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model9.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "113310fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.575\n",
      "precision: 0.5428571428571428\n",
      "recall:    0.95\n",
      "f1 score:  0.6909090909090909\n",
      "ROC AUC:   0.7025\n",
      "condusion matrix:\n",
      "   [[ 4 16]\n",
      " [ 1 19]]\n",
      "acc:  0.65\n",
      "precision: 0.75\n",
      "recall:    0.45\n",
      "f1 score:  0.5625\n",
      "ROC AUC:   0.66\n",
      "condusion matrix:\n",
      "   [[17  3]\n",
      " [11  9]]\n"
     ]
    }
   ],
   "source": [
    "def model_eval():    \n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in batch_generator(file_list_test, batch_size, scaler=scaler):\n",
    "            outputs = model(X_batch)\n",
    "            probs = outputs.squeeze(1).numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            y_prob.extend(probs.tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "            y_true.extend(y_batch.squeeze(1).numpy().tolist())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"acc:  {accuracy}\")\n",
    "    print(f\"precision: {precision}\")\n",
    "    print(f\"recall:    {recall}\")\n",
    "    print(f\"f1 score:  {f1}\")\n",
    "    print(f\"ROC AUC:   {roc_auc}\")\n",
    "    print(f\"condusion matrix:\\n   {cm}\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"model7.pth\"))\n",
    "model_eval()\n",
    "model.load_state_dict(torch.load(\"model8.pth\"))\n",
    "model_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b34f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 0 | Pred: 1 | Confidence: 0.5270\n",
      "True: 0 | Pred: 0 | Confidence: 0.4997\n",
      "True: 0 | Pred: 0 | Confidence: 0.4106\n",
      "True: 0 | Pred: 1 | Confidence: 0.5195\n",
      "True: 0 | Pred: 1 | Confidence: 0.5833\n",
      "True: 0 | Pred: 1 | Confidence: 0.5680\n",
      "True: 0 | Pred: 0 | Confidence: 0.4729\n",
      "True: 0 | Pred: 1 | Confidence: 0.5273\n",
      "True: 0 | Pred: 1 | Confidence: 0.5583\n",
      "True: 0 | Pred: 1 | Confidence: 0.6504\n",
      "True: 0 | Pred: 1 | Confidence: 0.5328\n",
      "True: 0 | Pred: 1 | Confidence: 0.6440\n",
      "True: 0 | Pred: 0 | Confidence: 0.4677\n",
      "True: 0 | Pred: 0 | Confidence: 0.4743\n",
      "True: 0 | Pred: 0 | Confidence: 0.4959\n",
      "True: 0 | Pred: 0 | Confidence: 0.4630\n",
      "True: 0 | Pred: 1 | Confidence: 0.5865\n",
      "True: 0 | Pred: 0 | Confidence: 0.4870\n",
      "True: 0 | Pred: 0 | Confidence: 0.4978\n",
      "True: 0 | Pred: 0 | Confidence: 0.4622\n",
      "True: 1 | Pred: 1 | Confidence: 0.5110\n",
      "True: 1 | Pred: 1 | Confidence: 0.6130\n",
      "True: 1 | Pred: 0 | Confidence: 0.4501\n",
      "True: 1 | Pred: 0 | Confidence: 0.4734\n",
      "True: 1 | Pred: 1 | Confidence: 0.5093\n",
      "True: 1 | Pred: 1 | Confidence: 0.6621\n",
      "True: 1 | Pred: 1 | Confidence: 0.5715\n",
      "True: 1 | Pred: 1 | Confidence: 0.5819\n",
      "True: 1 | Pred: 0 | Confidence: 0.4787\n",
      "True: 1 | Pred: 1 | Confidence: 0.5733\n",
      "True: 1 | Pred: 0 | Confidence: 0.4888\n",
      "True: 1 | Pred: 0 | Confidence: 0.4677\n",
      "True: 1 | Pred: 0 | Confidence: 0.4752\n",
      "True: 1 | Pred: 0 | Confidence: 0.4618\n",
      "True: 1 | Pred: 1 | Confidence: 0.5163\n",
      "True: 1 | Pred: 0 | Confidence: 0.4736\n",
      "True: 1 | Pred: 1 | Confidence: 0.5532\n",
      "True: 1 | Pred: 1 | Confidence: 0.5404\n",
      "True: 1 | Pred: 1 | Confidence: 0.5659\n",
      "True: 1 | Pred: 1 | Confidence: 0.5239\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in batch_generator(file_list_test, batch_size, scaler=scaler):\n",
    "        outputs = model(X_batch)\n",
    "        probs = outputs.squeeze(1).numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "        y_true_batch = y_batch.squeeze(1).numpy().astype(int)\n",
    "        for prob, pred, true_label in zip(probs, preds, y_true_batch):\n",
    "            print(f\"true: {true_label} | pred: {pred} | confidence: {prob:.4f}\")\n",
    "        y_prob.extend(probs.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_true.extend(y_true_batch.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76964636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "model 4:\n",
    "Test Accuracy:  0.6000\n",
    "Test Precision: 0.5588\n",
    "Test Recall:    0.9500\n",
    "Test F1-score:  0.7037\n",
    "Test ROC AUC:   0.6825\n",
    "Confusion Matrix:\n",
    "   [[ 5 15]\n",
    " [ 1 19]]\n",
    "\n",
    "\n",
    " model 7:\n",
    " Test Accuracy:  0.5750\n",
    "Test Precision: 0.5429\n",
    "Test Recall:    0.9500\n",
    "Test F1-score:  0.6909\n",
    "Test ROC AUC:   0.7025\n",
    "Confusion Matrix:\n",
    "   [[ 4 16]\n",
    " [ 1 19]]\n",
    "\n",
    "model 8:\n",
    "Test Accuracy:  0.6500\n",
    "Test Precision: 0.7500\n",
    "Test Recall:    0.4500\n",
    "Test F1-score:  0.5625\n",
    "Test ROC AUC:   0.6600\n",
    "Confusion Matrix:\n",
    "   [[17  3]\n",
    " [11  9]]\n",
    "\n",
    "model 7 + model 8:\n",
    "Test Accuracy:  0.6500\n",
    "Test Precision: 0.6250\n",
    "Test Recall:    0.7500\n",
    "Test F1-score:  0.6818\n",
    "Test ROC AUC:   0.7025\n",
    "Confusion Matrix:\n",
    "   [[11  9]\n",
    " [ 5 15]] \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "61d93a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model7: [0.5967618  0.5357842  0.44540623 0.5943625  0.56813085] Model8: [0.47916403 0.416531   0.39346752 0.4887952  0.4869161 ] Avg: [0.5379629  0.4761576  0.41943687 0.5415788  0.52752346]\n",
      "acc:  0.65\n",
      "precision: 0.625\n",
      "recall:    0.75\n",
      "f1 score:  0.6818181818181818\n",
      "ROC AUC:   0.7025\n",
      "condusion matrix:\n",
      "   [[11  9]\n",
      " [ 5 15]]\n"
     ]
    }
   ],
   "source": [
    "model7 = nn.Sequential(\n",
    "    nn.Linear(input_dim, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model8 = nn.Sequential(\n",
    "    nn.Linear(input_dim, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model7.load_state_dict(torch.load(\"model7.pth\"))\n",
    "model8.load_state_dict(torch.load(\"model8.pth\"))\n",
    "model7.eval()\n",
    "model8.eval()\n",
    "\n",
    "y_true, y_pred, y_prob = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in batch_generator(file_list_test, batch_size, scaler=scaler):\n",
    "        prob7 = model7(X_batch).squeeze(1).numpy()\n",
    "        prob8 = model8(X_batch).squeeze(1).numpy()\n",
    "        avg_prob = (prob7 + prob8) / 2\n",
    "        preds = (avg_prob >= 0.5).astype(int)\n",
    "        y_prob.extend(avg_prob.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_true.extend(y_batch.squeeze(1).numpy().tolist())\n",
    "        print(\"Model7:\", prob7[:5], \"Model8:\", prob8[:5], \"Avg:\", avg_prob[:5])\n",
    "        \n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"acc:  {accuracy}\")\n",
    "print(f\"precision: {precision}\")\n",
    "print(f\"recall:    {recall}\")\n",
    "print(f\"f1 score:  {f1}\")\n",
    "print(f\"ROC AUC:   {roc_auc}\")\n",
    "print(f\"condusion matrix:\\n   {cm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
